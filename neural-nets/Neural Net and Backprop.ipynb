{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(inputSignal):\n",
    "    return 1/(1 + (np.exp(-1 * inputSignal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dotProduct(numpyVector1,numpyVector2):\n",
    "    if not(isinstance(numpyVector1,np.ndarray)):\n",
    "        numpyVector1 = np.array(numpyVector1)\n",
    "    if not(isinstance(numpyVector2,np.ndarray)):\n",
    "        numpyVector2 = np.array(numpyVector2)\n",
    "    if numpyVector1.size != numpyVector2.size:\n",
    "        raise Exception(\"Dimensions do not match for dot product\")\n",
    "    else:\n",
    "        return np.dot(numpyVector1,numpyVector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class neuron(object):\n",
    "\n",
    "    # Defining a sigmoid neuron with n inputs and one output\n",
    "    def __init__(self, num_inputs, actFun = \"sigmoid\"):\n",
    "        # Intialize with random weights\n",
    "        self.num_inputs = num_inputs\n",
    "        self.weights = [rd.random() for i in (range(num_inputs + 1) )]\n",
    "        self.weights = np.array(self.weights)\n",
    "        self.actFun = actFun\n",
    "        self.output = None\n",
    "    \n",
    "    def forwardPropagate(self,inputVector):\n",
    "        \n",
    "        # Propagate the input and use activation function on the output        \n",
    "        propOut = dotProduct(self.weights, np.append(1,inputVector))\n",
    "        if self.actFun == \"sigmoid\":\n",
    "            self.output = sigmoid(propOut)\n",
    "        else:\n",
    "            print \"Activation function not found\"\n",
    "        \n",
    "        # Return the output to a function that can pass it on to next layer\n",
    "        return self.output\n",
    "        \n",
    "    def adjustWeights(self, weightUpdate):\n",
    "        \n",
    "        # weightUpdates are usually applied during backpropagation\n",
    "        # Weight updates are applied all n + 1 input lines\n",
    "        if(self.weights.size != weightUpdate.size):\n",
    "            if not(isinstance(weightUpdates,np.ndarray)):\n",
    "                weightUpdate = np.array(weightUpdate)\n",
    "            self.weights = np.add(self.weights,weightUpdate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-152-29c04bf266e7>, line 75)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-152-29c04bf266e7>\"\u001b[0;36m, line \u001b[0;32m75\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class neuralNet(object):\n",
    "    \n",
    "    # Defining a neural network that takes in the following inputs\n",
    "    # Number of neurons in each layer as a vector. An 8x3x1 neural net can be initialized with n = [8,3,1]\n",
    "    # Weights are randomly initialized for all neurons. Note: Later planning to add matrix inputs for custom weights\n",
    "    # Methods include \n",
    "    # 1. Forward propagation\n",
    "    # 2. Backprop training algorithm\n",
    "    \n",
    "    def __init__(self, n, actFun = \"sigmoid\"):\n",
    "        \n",
    "        if(not(isinstance(n,np.ndarray))):\n",
    "            n = np.array(n)\n",
    "            \n",
    "        self.n = n\n",
    "        \n",
    "        # Number of layers excluding the input layer\n",
    "        numLayers = n.size - 1\n",
    "        \n",
    "        # Check if n is of the right size (>=2)\n",
    "        if(n.size < 2 or n.ndim > 1):\n",
    "            raise Exception(\"Input vector is not the right size. Specify a 1D vector with at least 2 elements\")\n",
    "        \n",
    "        # Defining a neural net based on n\n",
    "        # Each layer has number of inputs = neurons in (n-1 layer)\n",
    "        # Can list comprehension be used here?\n",
    "        print \"Generating Neural Net..!\"\n",
    "        \n",
    "        # Empty net soon to be populated with randomly initialized neurons\n",
    "        self.net = []\n",
    "        \n",
    "        for i in range(numLayers):\n",
    "            numInputs = n[i]\n",
    "            numNeurons = n[i+1]\n",
    "            neuronLayer = [neuron(numInputs, actFun) for x in range(numNeurons)]\n",
    "            self.net.append(neuronLayer)\n",
    "            \n",
    "        self.output = [rd.random() for x in range(n[-1])]\n",
    "        \n",
    "        print \"Generated...!\"\n",
    "        \n",
    "    \n",
    "    # Defining the forward propagation algorithm for each layer\n",
    "    # Output of each neeuron is a dot product of each neuron's weight and the input\n",
    "    # The dot product is processed by the activation function to provide the final output\n",
    "    def forwardPropagate(self,inputVector):\n",
    "        \n",
    "        # The input vector is assumed to be the same size as the expect inputs for the neuron\n",
    "        # If not the dot product wrapper function dotProduct() raises an exception\n",
    "        # For each neuron layerwise, this function propagates the input vector by calling\n",
    "        # the forwardPropagate() of each neuron object\n",
    "        \n",
    "        layerInput = inputVector\n",
    "        \n",
    "        for layer in self.net:\n",
    "            layerOutput = [nrn.forwardPropagate(layerInput) for nrn in layer]\n",
    "            layerInput = layerOutput\n",
    "    \n",
    "    def getOutputs(self):\n",
    "        \n",
    "        # Return output of each neuron in last layer\n",
    "        return [nrn.output for nrn in self.net[-1]]\n",
    "    \n",
    "    def getWeights(self):\n",
    "        \n",
    "        # Return weights of each neuron layerwise as a matrix\n",
    "        return [[nrn.weights for nrn in layer] for layer in self.net]\n",
    "        \n",
    "    def backprop(self,trainData,trainTarget, learningRate, method = \"stochastic\"):\n",
    "        \n",
    "        # The standard back prop algorithm that trains each neuron by\n",
    "        # propagating the error at the output stagewise backwards\n",
    "        # If t is the training target vector and o is the output observed:\n",
    "        # The output layer weights are adjusted by the quantity:\n",
    "        # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Neural Net..!\n",
      "Generated...!\n"
     ]
    }
   ],
   "source": [
    "nn1 = neuralNet([3,3,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn1.forwardPropagate([1 for i in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.88611471669524799]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn1.getOutputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wts = nn1.getWeights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[array([ 0.28959358,  0.47521881,  0.95263676,  0.03651433]), array([ 0.16824114,  0.44676349,  0.91661645,  0.1472302 ]), array([ 0.40923739,  0.38629785,  0.1146397 ,  0.89741689])], [array([ 0.79466873,  0.11857628,  0.78693762,  0.57354067])]]\n"
     ]
    }
   ],
   "source": [
    "print wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85907057306770629"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(wts[0][2].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78259443049074917"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(0.32 + 0.85 * 0.08 + 0.95 * 0.705 + 0.97 * 0.23)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
