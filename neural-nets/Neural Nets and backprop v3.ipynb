{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lookup table of activation functions\n",
    "actLookup = {\"sigmoid\":lambda x: 1/(1 + np.exp(-1 * x)),\n",
    "                         \"linear\":lambda x: x}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class layer(object):\n",
    "    # A neural net layer with n neurons as defined by the user and an activation function for the layer\n",
    "    \n",
    "    def __init__(self, numInputs, layerSize, actFun = \"sigmoid\"):\n",
    "        \n",
    "        # Size of the layer and the number of inputs\n",
    "        self.layerSize = layerSize\n",
    "        self.actFun = actFun\n",
    "        # Incorporating the bias neuron\n",
    "        self.numInputs = numInputs + 1\n",
    "        self.inputMatrix = None\n",
    "        \n",
    "        # Numpy matrix of weights\n",
    "        self.weights = np.array([[rd.random() for i in range(self.numInputs)] \n",
    "                                 for j in range(self.layerSize)])\n",
    "        \n",
    "        # Set the backward propagation values for this layer\n",
    "        self.delta = None\n",
    "        \n",
    "    # Defining the forward function to the layer\n",
    "    def forward(self, inputMatrix):\n",
    "        \"\"\"Forward() forward propagates the inputs to a layer\"\"\"\n",
    "        \n",
    "        # Convert to numpy array if not passed as a numpy array\n",
    "        if(not(isinstance(inputMatrix,np.ndarray))):\n",
    "            inputMatrix = np.array(inputMatrix)\n",
    "        \n",
    "        rows, columns = inputMatrix.shape\n",
    "        \n",
    "        # Dot product of input matrix (with extra 1s for the bias neuron) with the weight matrix\n",
    "        inputPadded = np.append(np.ones(rows).reshape(rows,1), inputMatrix, axis = 1)\n",
    "        self.inputMatrix = inputPadded\n",
    "        layerOutput = np.dot(inputPadded, self.weights.T)\n",
    "        \n",
    "        # Pass through activation function\n",
    "        self.output = actLookup[self.actFun](layerOutput)\n",
    "        return self.output\n",
    "    \n",
    "    # Defining the backward propagation function\n",
    "    def backward(self, trainTarget, delta):\n",
    "        \"\"\"Backward takes the delta from next layer and passes it on the previous layer\"\"\"\n",
    "        \n",
    "        if(not(isinstance(trainTarget,np.ndarray))):\n",
    "            trainTarget = np.array(trainTarget)\n",
    "        \n",
    "        # If delta is none then this layer is an output layere\n",
    "        if(delta is None):\n",
    "            \n",
    "            rows, columns = trainTarget.shape\n",
    "            errorTerm = np.subtract(trainTarget, self.output)\n",
    "            if self.actFun == \"sigmoid\":\n",
    "                outputInvert = np.array([(1 - op) for op in self.output])\n",
    "                term1 = np.multiply(outputInvert, self.output)\n",
    "                term2 = np.multiply(errorTerm, term1)\n",
    "                self.delta  = term2\n",
    "    \n",
    "\n",
    "        # If delta has been passed on to this layer, then this layer is a hidden layer\n",
    "        else:\n",
    "            \n",
    "            # Delta passed back is wkh * deltak for every batch instance\n",
    "            # Delta for each batch element is sum by column of the delta matrix passed back\n",
    "            # by the next layer\n",
    "            deltaTerm3 = delta.sum(axis = 1)\n",
    "            \n",
    "            # note: need to add derivatives for other activation functions\n",
    "            if self.actFun == \"sigmoid\":\n",
    "                    \n",
    "                outputInvert = np.array([(1 - op) for op in self.output])\n",
    "                # Term1 represents o(1-o)\n",
    "                term1 = np.multiply(outputInvert, self.output)\n",
    "                self.delta = np.multiply(term1,deltaTerm3)\n",
    "        \n",
    "        # Delta to pass on to preceding layer has to be \n",
    "        # matrix multiplication of these values by the weight matrix\n",
    "        weightMatrix = self.weights[:,1:]\n",
    "        deltaBack = np.array([np.multiply(deltaInst.T,weightMatrix) for deltaInst in self.delta])\n",
    "        return deltaBack\n",
    "    \n",
    "    def updateWeights(self, learnRate):\n",
    "        \"\"\"updateWeights() uses the inputs and the delta stored in each layer after forward\n",
    "        and backward propagation to derive the weight update rule\"\"\"\n",
    "        \n",
    "        # The udpate rule for each element in the matrix is given by\n",
    "        # wi = wi + sum_over_instances(delta for neuron * input i to neuron * learning rate)\n",
    "        # Compute the weight updates for every data point and then add those updates\n",
    "        rows,columns = self.inputMatrix.shape\n",
    "        weightUpdates = np.array(\n",
    "            [np.multiply(self.inputMatrix[i],\n",
    "                         self.delta[i].reshape(self.layerSize,1))\n",
    "             for i in range(rows)]).sum(axis = 0) * learnRate\n",
    "        self.weights = np.add(self.weights,weightUpdates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class neuralNet(object):\n",
    "    \n",
    "    \"\"\"Neural net object is a combination of layer objects. Has 2 functions\n",
    "    predict and backprop\"\"\"\n",
    "    \n",
    "    def __init__(self, layerList, actFun = \"sigmoid\"):\n",
    "        \n",
    "        \"\"\"Takes in the layerlist as input and generates as many layers\"\"\"\n",
    "        \n",
    "        # Activations to be used in the net\n",
    "        self.actFun = actFun\n",
    "        \n",
    "        # Creating the layers\n",
    "        self.layers = [layer(numInputs=layerList[i - 1],\n",
    "                             layerSize=layerList[i],                            \n",
    "                             actFun=self.actFun) for i in range(1, len(layerList))]\n",
    "        \n",
    "    def getWeights(self):\n",
    "        return [layer.weights for layer in self.layers]\n",
    "        \n",
    "    def predict(self, inputMatrix):\n",
    "        \"\"\"Predict function is used to propagate the inputs from \n",
    "        one layer to the next and get the final output from the layer\"\"\"\n",
    "        # Pass on input of previous layer to the next\n",
    "        # If 1st hidden layer, pass on the input\n",
    "        layerOut = inputMatrix\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            layerOut = layer.forward(inputMatrix=layerOut)\n",
    "            \n",
    "        return layerOut\n",
    "    \n",
    "    def backprop(self, trainInput, trainOutput, learnRate, batchSize = 1, nIter = 100):\n",
    "        \"\"\"Back prop is used to update the weights based on the training sample\n",
    "        It runs nIter iterations on the trainInput with batchSize number of rows\n",
    "        Weight updates are carried out at learning rate learnRate\"\"\"\n",
    "        \n",
    "        if(not(isinstance(trainOutput,np.ndarray))):\n",
    "            trainOutput = np.array(trainOutput)\n",
    "        if(not(isinstance(trainInput,np.ndarray))):\n",
    "            trainInput = np.array(trainInput)\n",
    "        \n",
    "        rows, columns = trainInput.shape\n",
    "        \n",
    "        for i in range(nIter):\n",
    "            \n",
    "            # Pick a random sample from the trainInput and trainOutput\n",
    "            # Updated weights based on the same. Sample size is to be of size batchSize\n",
    "            \n",
    "            randomIndices = np.random.choice(range(rows),size=batchSize)\n",
    "            \n",
    "            # Sample from trainInput and trainOutput\n",
    "            batchTrain = trainInput[randomIndices,:].reshape(batchSize,columns)\n",
    "            batchTest = trainOutput[randomIndices,:]\n",
    "            \n",
    "            # A forward pass through the network\n",
    "            self.predict(batchTrain)\n",
    "            \n",
    "            # Iterate backwards through the layers to pass the deltas\n",
    "            delta = None\n",
    "            \n",
    "            for layer in self.layers[::-1]:\n",
    "                \n",
    "                delta = layer.backward(trainTarget=batchTest,delta=delta)\n",
    "            \n",
    "                layer.updateWeights(learnRate=learnRate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
