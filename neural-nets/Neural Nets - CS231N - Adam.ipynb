{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO-DO\n",
    "- <strike>CV Error plots</strike>\n",
    "- <strike>Zero centering of data</strike>\n",
    "- <strike>L1/L2 regularization</strike>\n",
    "- <strike>Xavier initialization (to avoid dead neurons in Sigmoids/Tanh)</strike>\n",
    "- <strike>He et.al initiliazation (to avoid dead neurons in ReLUs)</strike>\n",
    "- <strike>Momentum/Nesterov Momentum</strike>\n",
    "- Adam\n",
    "- Benchmark current Algorithm on CIFAR - 10 dataset\n",
    "- Dropout (Inverted Dropout)\n",
    "- Batch Norm\n",
    "- HyperOpt\n",
    "- Convolution Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "from matplotlib import pyplot as plt\n",
    "from mnist import MNIST as mn\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def centerData2(inputMatrix):\n",
    "    \n",
    "    \"\"\"Center each variable to 0\"\"\"\n",
    "    # (x - mu)/sigma. Data is now centered at 0\n",
    "    \n",
    "    # Calculate mean and variance for each input feature\n",
    "    variableMeans = np.mean(inputMatrix, axis = 0)\n",
    "    \n",
    "    # Centering with mean and return\n",
    "    centerData = inputMatrix - variableMeans\n",
    "    return centerData,variableMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lookup table of activation functions and derivatives\n",
    "actLookup = {\"sigmoid\":lambda x: 1/(1 + np.exp(-1 * x)),\n",
    "             \"tanh\":lambda x: np.tanh(x),\n",
    "            \"relu\": lambda x: np.maximum(0,x)}\n",
    "\n",
    "derivativeLookup = {\"sigmoid\": lambda x: x * (1.-x),\n",
    "               \"tanh\": lambda x: 1. - (np.tanh(x) ** 2),\n",
    "              \"relu\": lambda x: np.ceil(x.clip(0,1))}\n",
    "\n",
    "# Weight Initialization strategy\n",
    "# Random multiplies the initializations by a small number\n",
    "# Xavier 1/sqrt(numInputs)\n",
    "# For relu's, initalization is as recommended by He et. al(2015)\n",
    "weightLookup = {\"random\":lambda x:0.01,\n",
    "               \"xavier\":lambda x:1/sqrt(x),\n",
    "               \"he.et.al\":lambda x:2/np.sqrt(x)}\n",
    "\n",
    "# Regularization error component,\n",
    "# 0 if none, sum(x^2) if L2 and sum(|x|) if L1\n",
    "regLookup = {\"none\":lambda x: 0,\n",
    "            \"l2\": lambda x: 0.5 * (x ** 2).sum(),\n",
    "            \"l1\" : lambda x:sum(abs(x))}\n",
    "\n",
    "# Derivatives of regularization terms\n",
    "regDerivLookup = {\"none\": lambda x:0,\n",
    "                \"l2\": lambda x: x,\n",
    "                \"l1\": lambda x: 1}\n",
    "\n",
    "# Momentum rate update\n",
    "momentumLookup = {\"none\":0,\n",
    "                  \"momentum\":lambda x:1,\n",
    "                  \"nag\": lambda x: (1+x)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class layer(object):\n",
    "    # A neural net layer with n neurons as defined by the user and an activation function for the layer\n",
    "    \n",
    "    def __init__(self,numInputs, layerSize, actFun = \"sigmoid\", weightInit = \"random\"):\n",
    "    \n",
    "        # Size of the layer and the number of inputs\n",
    "        self.layerSize = layerSize\n",
    "        self.actFun = actLookup[actFun]\n",
    "        self.deriv = derivativeLookup[actFun]\n",
    "\n",
    "        # Incorporating the bias neuron\n",
    "        self.numInputs = numInputs + 1\n",
    "        self.inputMatrix = None\n",
    "        \n",
    "        # Numpy matrix of weights\n",
    "        np.random.seed(seed=10000)\n",
    "        self.weights = np.array(np.random.randn(self.layerSize,self.numInputs))/100 * \\\n",
    "        weightLookup[weightInit](self.numInputs)\n",
    "        \n",
    "        # Set the backward propagation values for this layer\n",
    "        self.delta = None\n",
    "        \n",
    "        # Various components of learning methods (Momentum/NAG)\n",
    "        self.velocity = np.zeros(self.weights.shape)\n",
    "        self.vPrev = self.velocity\n",
    "        self.m = np.zeros(self.weights.shape)\n",
    "               \n",
    "        \n",
    "    # Defining the forward function to the layer\n",
    "    def forward(self, inputMatrix):\n",
    "        \"\"\"Forward() forward propagates the inputs to a layer\"\"\"\n",
    "        \n",
    "        # Convert to numpy array if not passed as a numpy array\n",
    "        if(not(isinstance(inputMatrix,np.ndarray))):\n",
    "            inputMatrix = np.array(inputMatrix)\n",
    "        \n",
    "        rows, columns = inputMatrix.shape\n",
    "        \n",
    "        # Dot product of input matrix (with extra 1s for the bias neuron) with the weight matrix\n",
    "        inputPadded = np.append(np.ones(rows).reshape(rows,1), inputMatrix, axis = 1)\n",
    "        self.inputMatrix = inputPadded\n",
    "        layerOutput = np.dot(inputPadded, self.weights.T)\n",
    "        \n",
    "        # Pass through activation function\n",
    "        self.output = self.actFun(layerOutput)\n",
    "        return self.output\n",
    "    \n",
    "    # Defining the backward propagation function\n",
    "    def backward(self, delta):\n",
    "        \"\"\"Backward takes the delta from next layer and passes it on the previous layer\"\"\"        \n",
    "\n",
    "        # Delta passed back is wkh * deltak for every batch instance\n",
    "        # Delta for each batch element is sum by column of the delta matrix passed back\n",
    "        # by the next layer\n",
    "\n",
    "        self.delta = np.multiply(self.deriv(self.output),delta)\n",
    "        \n",
    "        # Delta to pass on to preceding layer has to be \n",
    "        # matrix multiplication of these values by the weight matrix\n",
    "        weightMatrix = self.weights[:,1:]\n",
    "        \n",
    "        # deltaBack = weightMatrix.T.dot(self.delta.T)\n",
    "        deltaBack = self.delta.dot(weightMatrix)\n",
    "        return deltaBack\n",
    "    \n",
    "    def updateWeights(self, learnRate,\n",
    "                      regularize = \"none\", lambdaReg = 0.01,\n",
    "                      learningMethod = \"none\", mu = 0.9,\n",
    "                      adam = 0, beta1 = 0.9, beta2 = 0.999, eps = 1e-8):\n",
    "        \"\"\"updateWeights() uses the inputs and the delta stored in each layer after forward\n",
    "        and backward propagation to derive the weight update rule\"\"\"\n",
    "        \n",
    "        regDeriv = regDerivLookup[regularize]\n",
    "        # The udpate rule for eachelement in the matrix is given by\n",
    "        # wi = wi + sum_over_instances(delta for neuron * input i to neuron * learning rated)\n",
    "        # Compute the weight updates for every data point and then add those updates\n",
    "        rows,columns = self.inputMatrix.shape\n",
    "        stepGradient = -1. * np.dot(self.inputMatrix.T,self.delta).T * (1/np.float(rows))\n",
    "        \n",
    "        # Regularization if applicable\n",
    "        stepGradient = np.add(stepGradient, lambdaReg * regDeriv(self.weights))\n",
    "        \n",
    "        # Weight updates for adam\n",
    "        self.m = beta1 * np.float(adam) * self.m \\\n",
    "        + ((1. - beta1) * adam + (1. - adam) * (-learnRate)) * stepGradient\n",
    "        \n",
    "        weightUpdates = self.m\n",
    "                \n",
    "        # Different learning methods\n",
    "        self.vPrev = self.velocity\n",
    "    \n",
    "        # Momentum using adam\n",
    "        self.velocity = (beta2 * adam + (1.-adam) * mu) * self.velocity \\\n",
    "        + (1. - beta2) * adam * (stepGradient ** 2) \\\n",
    "        + (1. - adam) * weightUpdates\n",
    "\n",
    "        # Change weightUpdates based on the learning rate\n",
    "        weightUpdates = (1. - adam) * \\\n",
    "        np.add(weightUpdates, momentumLookup[learningMethod](mu)  * self.velocity)\n",
    "        \n",
    "        # If NAG is chosen, additional term gets added\n",
    "        weightUpdates = (1. - adam) * \\\n",
    "        np.subtract(weightUpdates, (mu if learningMethod == \"nag\" else 0.) * self.vPrev)\n",
    "        \n",
    "        # If adam update is chosen\n",
    "        adamUpdate = -adam * learnRate * self.m / (np.sqrt(np.float(adam) * self.velocity) + eps)\n",
    "        \n",
    "        # Final Weight updates\n",
    "        self.weights = np.add(self.weights, np.float(adam) * adamUpdate + (1. - adam) * weightUpdates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class neuralNet(object):\n",
    "    \n",
    "    \"\"\"Neural net object is a combination of layer objects. Has 2 functions\n",
    "    predict and backprop\"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "    def __init__(self, layerList, actFun = \"sigmoid\", weightInit = \"random\"):\n",
    "        \n",
    "        \"\"\"Takes in the layerlist as input and generates as many layers\"\"\"\n",
    "        \n",
    "        self.actFun = actFun\n",
    "        \n",
    "        # Creating the layers\n",
    "        self.layers = [layer(numInputs=layerList[i - 1],\n",
    "                             layerSize=layerList[i],                            \n",
    "                             actFun=self.actFun,\n",
    "                            weightInit = weightInit) for i in range(1, len(layerList))]\n",
    "        self.cvError = None\n",
    "        self.regLoss= None\n",
    "        \n",
    "    def predict(self, inputMatrix):\n",
    "        \"\"\"Predict function is used to propagate the inputs from \n",
    "        one layer to the next and get the final output from the layer\"\"\"\n",
    "        # Pass on input of previous layer to the next\n",
    "        # If 1st hidden layer, pass on the input\n",
    "        layerOut = inputMatrix\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            layerOut = layer.forward(inputMatrix=layerOut)\n",
    "            \n",
    "        return layerOut\n",
    "    \n",
    "    def computeError(self, inputs, outputs):\n",
    "        \n",
    "        predictions = self.predict(inputs)\n",
    "        numRows = inputs.shape[0]\n",
    "        # Calculate deltas and sum squares\n",
    "        delta = np.subtract(predictions,outputs)\n",
    "        totalError = np.sum(delta**2)/numRows\n",
    "        return totalError\n",
    "    \n",
    "    def computeRegularizationLoss(self, regularize, lambdaReg):\n",
    "        # Calculate layerWise sums and get total\n",
    "        regLossFunction  = regLookup[regularize]       \n",
    "        layerRegs = np.array([lambdaReg * regLossFunction(layer.weights) for layer in self.layers])\n",
    "        return(layerRegs.sum())\n",
    "    \n",
    "    def backprop(self, trainInput, trainOutput, learnRate, \n",
    "                 testInput,testOutput,\n",
    "                 batchSize = 1, nIter = 100, \n",
    "                 showStep = 1000, stepError = 1000,\n",
    "                 regularize = \"none\",lambdaReg = 0.01,\n",
    "                 learningMethod = \"none\", mu = 0.9,\n",
    "                 adam = 0, beta1 = 0.9, beta2 = 0.999, eps = 1e-8):\n",
    "        \n",
    "        \"\"\"Back prop is used to update the 2000weights based on the training sample\n",
    "        It runs nIter iterations on the trainInput with batchSize number of rows\n",
    "        Weight updates are carried out at learning rate learnRate\"\"\"\n",
    "        \n",
    "        if(not(isinstance(trainOutput,np.ndarray))):\n",
    "            trainOutput = np.array(trainOutput)\n",
    "        if(not(isinstance(trainInput,np.ndarray))):\n",
    "            trainInput = np.array(trainInput)\n",
    "        \n",
    "        rows, columns = trainInput.shape\n",
    "        \n",
    "        # initialize empty CV error matrix: Column 1 represents Train Error\n",
    "        # Column 2 represents Test Error\n",
    "        self.cvError = np.zeros(nIter/stepError * 2).reshape(nIter/stepError,2)\n",
    "        \n",
    "        self.regLoss = np.zeros(nIter/stepError * 2).reshape(nIter/stepError,2)\n",
    "        \n",
    "        for i in range(nIter):\n",
    "            \n",
    "            if((i + 1) % showStep == 0):\n",
    "                print i + 1\n",
    "                \n",
    "            if((i + 1) % stepError == 0):\n",
    "                self.cvError[((i + 1) / stepError)-1] = [self.computeError(trainInput,trainOutput),\n",
    "                                   self.computeError(testInput,testOutput)]\n",
    "                \n",
    "                self.regLoss[((i + 1) / stepError)-1] = [self.computeRegularizationLoss(regularize = regularize,\n",
    "                                                                                        lambdaReg = lambdaReg)]\n",
    "            \n",
    "            # Pick a random sample from the trainInput and trainOutput\n",
    "            # Updated weights based on the same. Sample size is to be of size batchSize\n",
    "            \n",
    "            randomIndices = np.random.choice(range(rows),size=batchSize)\n",
    "            \n",
    "            # Sample from trainInput and trainOutput\n",
    "            batchTrain = trainInput[randomIndices,:].reshape(batchSize,columns)\n",
    "            batchTest = trainOutput[randomIndices,:]\n",
    "        \n",
    "            # A forward pass through the network\n",
    "            output = self.predict(batchTrain)\n",
    "            \n",
    "            # Iterate backwards through the layers to pass the deltas\n",
    "            delta = np.subtract(batchTest,output)\n",
    "            \n",
    "            for layer in self.layers[::-1]:\n",
    "                \n",
    "                # Delta to be passed to the previous layer is computed\n",
    "                delta = layer.backward(delta=delta)\n",
    "                # Update weights as determined by the delta gradient\n",
    "                layer.updateWeights(learnRate=learnRate, \n",
    "                                    regularize = regularize, lambdaReg = lambdaReg,\n",
    "                                    learningMethod = learningMethod, mu = mu,\n",
    "                                    adam = adam, beta1 = beta1, beta2 = beta2, eps = eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mdata = mn(path='../datasets/MNIST/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images,labels = mdata.load_training()\n",
    "images = np.array(images)/255.\n",
    "labels = np.array(labels).reshape(60000,1).astype(dtype = 'uint8')\n",
    "labels = np.unpackbits(labels,axis=1)\n",
    "labels = labels[:,4:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampleIndices = np.random.choice(range(len(images)),size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imageTrain = images[sampleIndices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelTrain = labels[sampleIndices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testImages,testLabels = mdata.load_testing()\n",
    "testImages = np.array(testImages)/255.\n",
    "testLabelsDigit = np.array(testLabels).reshape(10000,1).astype(dtype = 'uint8')\n",
    "testLabels = np.unpackbits(testLabelsDigit,axis=1)\n",
    "testLabels = testLabels[:,4:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampleTestIndices = np.random.choice(range(len(testImages)), 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imageTest = testImages[sampleTestIndices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labelTestDigit = testLabelsDigit[sampleTestIndices]\n",
    "labelTest = testLabels[sampleTestIndices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Not improving accuracy\n",
    "# centerTrain, trainMeans = centerData2(imageTrain)\n",
    "# centerTest = imageTest - trainMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training a neural net with 256 inputs, 300 hidden units (1 layer only)\n",
    "nn3 = neuralNet(layerList=[784,300,4],actFun=\"relu\",weightInit=\"he.et.al\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "nn3.backprop(batchSize=10,learnRate=0.002,nIter=10000,showStep=2000,trainInput=imageTrain,\n",
    "             stepError = 40, trainOutput=labelTrain,testInput = imageTest,testOutput = labelTest,\n",
    "             regularize = \"l2\", lambdaReg = 1e-5,learningMethod = \"nag\", mu = 0.9, adam = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute accuracy of digit recognition\n",
    "preds = nn3.predict(imageTest)\n",
    "preds = np.array([np.round(arr) for arr in preds])\n",
    "preds = np.append(np.zeros(8000).reshape(2000,4),preds,axis=1)\n",
    "predDigits = np.packbits(preds.astype(\"bool\"))\n",
    "acc = [predDigits[i] == labelTestDigit[i] for i in range(2000)]\n",
    "print sum(acc)/2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(nn3.cvError[:,0],'r',label = 'train')\n",
    "plt.plot(nn3.cvError[:,1],'g',label = 'test')\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
